{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BHs8rxGj5tmrk0uGJDP2-v7dewj2sK81",
      "authorship_tag": "ABX9TyPJSnrb7Y0V7vsXQHnf41g4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaibhavSingh1311/Syngas-Prediction-from-Biomass-FeedStock/blob/main/DataSet_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic data processing and ML libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Install required packages for synthetic data\n",
        "!pip install sdv\n",
        "!pip install table-evaluator\n",
        "\n",
        "from sdv.single_table import CTGANSynthesizer, GaussianCopulaSynthesizer\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.evaluation.single_table import evaluate_quality, run_diagnostic\n",
        "\n",
        "from table_evaluator import TableEvaluator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ob5SG-bxG-6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Prepare Your Data\n",
        "\n",
        "original_data = pd.read_csv('/content/drive/MyDrive/ML_Lab_Datasets/Training dataset.csv', encoding='utf-8-sig')\n",
        "\n",
        "print(f\"\\nOriginal data shape: {original_data.shape}\")\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "data_clean = original_data.copy()\n",
        "\n",
        "# Handle missing values\n",
        "for col in data_clean.columns:\n",
        "    if data_clean[col].isnull().sum() > 0:\n",
        "        print(f\"Column '{col}' has {data_clean[col].isnull().sum()} missing values\")\n",
        "        # Fill numeric columns with median\n",
        "        if data_clean[col].dtype in ['float64', 'int64']:\n",
        "            data_clean[col].fillna(data_clean[col].median(), inplace=True)\n",
        "        else:\n",
        "            # Fill categorical columns with mode\n",
        "            if len(data_clean[col].mode()) > 0:\n",
        "                data_clean[col].fillna(data_clean[col].mode()[0], inplace=True)\n",
        "            else:\n",
        "                data_clean[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "print(f\"\\nData after cleaning: {data_clean.shape}\")\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(data_clean.isnull().sum())\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = data_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = data_clean.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# Check for outliers in numerical columns\n",
        "print(\"\\nChecking for outliers (beyond 3 std dev):\")\n",
        "outlier_info = {}\n",
        "for col in numerical_cols:\n",
        "    mean = data_clean[col].mean()\n",
        "    std = data_clean[col].std()\n",
        "    outliers = data_clean[(data_clean[col] < mean - 3*std) | (data_clean[col] > mean + 3*std)]\n",
        "    outlier_info[col] = len(outliers)\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"  {col}: {len(outliers)} outliers\")\n",
        "\n",
        "# Normalize gas composition if present\n",
        "gas_cols = ['H2', 'CO', 'CO2', 'CH4']\n",
        "if all(col in data_clean.columns for col in gas_cols):\n",
        "    data_clean['Gas_Sum'] = data_clean[gas_cols].sum(axis=1)\n",
        "    print(f\"\\nOriginal gas composition sum - Mean: {data_clean['Gas_Sum'].mean():.1f}%\")\n",
        "\n",
        "    # Normalize gas percentages\n",
        "    for col in gas_cols:\n",
        "        data_clean[col] = data_clean[col] / data_clean['Gas_Sum'] * 100\n",
        "\n",
        "    data_clean.drop('Gas_Sum', axis=1, inplace=True)\n",
        "\n",
        "# Display biomass type statistics\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"BIOMASS TYPE STATISTICS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(f\"Number of unique biomass types: {data_clean['TYPES'].nunique()}\")"
      ],
      "metadata": {
        "id": "nmw-v6NzIaAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Metadata Creation with Constraints\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING ENHANCED METADATA WITH CONSTRAINTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Automatically detect metadata from the dataframe\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=data_clean)\n",
        "\n",
        "print(\"\\nDetected metadata:\")\n",
        "for column in metadata.columns:\n",
        "    col_info = metadata.columns[column]\n",
        "    print(f\"  {column}: {col_info['sdtype']}\")\n",
        "\n",
        "# Prepare constraints for the synthesizer\n",
        "constraints = []\n",
        "\n",
        "# Add constraints for gas compositions to keep values between 0-100%\n",
        "gas_cols = ['H2', 'CO', 'CO2', 'CH4']\n",
        "for col in gas_cols:\n",
        "    if col in data_clean.columns:\n",
        "        # Get current range from the data\n",
        "        min_val = data_clean[col].min()\n",
        "        max_val = data_clean[col].max()\n",
        "\n",
        "        # Create constraint range with 10% buffer\n",
        "        constraint_min = max(0, min_val * 0.9)\n",
        "        constraint_max = min(100, max_val * 1.1)\n",
        "\n",
        "        # Add constraint only if it's valid\n",
        "        if constraint_max > constraint_min:\n",
        "            print(f\"  {col}: Will constrain to range [{constraint_min:.1f}, {constraint_max:.1f}]\")\n",
        "\n",
        "print(\"\\nEnhanced metadata created successfully!\")\n",
        "print(f\"Total columns: {len(metadata.columns)}\")"
      ],
      "metadata": {
        "id": "dd5d0fdhIf6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Synthesizer Approach for Biomass Data\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CORRECTED SYNTHESIZER APPROACH FOR BIOMASS DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# First, analyze the dataset to understand its structure\n",
        "n_samples = len(data_clean)\n",
        "n_features = len(data_clean.columns)\n",
        "n_categories = data_clean['TYPES'].nunique()\n",
        "min_class_size = data_clean['TYPES'].value_counts().min()\n",
        "\n",
        "print(f\"\\nDataset Analysis:\")\n",
        "print(f\"  - Total samples: {n_samples}\")\n",
        "print(f\"  - Features: {n_features}\")\n",
        "print(f\"  - Biomass types: {n_categories}\")\n",
        "print(f\"  - Smallest class: {min_class_size} samples\")\n",
        "\n",
        "# For biomass data, some properties are fixed for each type\n",
        "# These don't change even when conditions vary\n",
        "fixed_properties = ['C', 'H', 'O', 'Ash', 'N', 'S', 'Moisture', 'Volatile matter', 'Fixed carbon']\n",
        "# Only include columns that actually exist in the data\n",
        "fixed_properties = [col for col in fixed_properties if col in data_clean.columns]\n",
        "\n",
        "print(f\"\\nFIXED PROPERTIES (do not vary within same biomass type):\")\n",
        "print(f\"  {fixed_properties}\")\n",
        "\n",
        "print(f\"\\nVARYING PROPERTIES (can vary within same biomass type):\")\n",
        "varying_properties = [col for col in data_clean.columns if col not in fixed_properties + ['TYPES']]\n",
        "print(f\"  {varying_properties}\")\n",
        "\n",
        "\n",
        "# Use Gaussian Copula synthesizer - works better with our dataset size\n",
        "synthesizer = GaussianCopulaSynthesizer(\n",
        "    metadata=metadata,\n",
        "    enforce_min_max_values=True,  # Keep values within observed ranges\n",
        "    enforce_rounding=False\n",
        ")\n",
        "\n",
        "synthesizer.fit(data_clean)\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "MVdbVx88If9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Smart Data Augmentation for Biomass Data\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SMART DATA AUGMENTATION FOR BIOMASS GASIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "target_total = 1000\n",
        "\n",
        "print(f\"\\nStarting with {len(data_clean)} original samples...\")\n",
        "\n",
        "# Group by biomass type\n",
        "type_groups = {}\n",
        "for biomass_type in data_clean['TYPES'].unique():\n",
        "    type_data = data_clean[data_clean['TYPES'] == biomass_type]\n",
        "    type_groups[biomass_type] = len(type_data)\n",
        "\n",
        "print(f\"\\nBiomass types and experiment counts:\")\n",
        "for biomass_type, count in type_groups.items():\n",
        "    print(f\"  {biomass_type}: {count} experiments\")\n",
        "\n",
        "# Calculate physical relationships\n",
        "print(\"\\nCalculating physical relationships...\")\n",
        "\n",
        "relationship_patterns = {}\n",
        "for biomass_type, type_data in data_clean.groupby('TYPES'):\n",
        "    if len(type_data) >= 2:\n",
        "        patterns = {}\n",
        "        if 'Temperature (°C)' in type_data.columns and 'H2' in type_data.columns:\n",
        "            patterns['temp_h2_slope'] = type_data['Temperature (°C)'].corr(type_data['H2'])\n",
        "        if 'H2' in type_data.columns and 'CO' in type_data.columns:\n",
        "            patterns['h2_co_slope'] = type_data['H2'].corr(type_data['CO'])\n",
        "        relationship_patterns[biomass_type] = patterns\n",
        "\n",
        "# Generate augmented samples\n",
        "augmented_samples_needed = target_total - len(data_clean)\n",
        "print(f\"\\nGenerating {augmented_samples_needed} augmented samples...\")\n",
        "\n",
        "augmented_rows = []\n",
        "for _ in range(augmented_samples_needed):\n",
        "    original_idx = np.random.randint(0, len(data_clean))\n",
        "    original_row = data_clean.iloc[original_idx].copy()\n",
        "    augmented_row = original_row.copy()\n",
        "\n",
        "    # Temperature variation\n",
        "    if 'Temperature (°C)' in augmented_row:\n",
        "        temp = augmented_row['Temperature (°C)']\n",
        "        temp_variation = np.random.normal(0, 25)\n",
        "        new_temp = temp + temp_variation\n",
        "        augmented_row['Temperature (°C)'] = max(500, min(950, new_temp))\n",
        "\n",
        "        # Adjust gas compositions\n",
        "        temp_change_pct = (new_temp - temp) / max(temp, 1)\n",
        "\n",
        "        if 'H2' in augmented_row and 'CO' in augmented_row:\n",
        "            h2_change = temp_change_pct * 0.3 * augmented_row['H2']\n",
        "            augmented_row['H2'] = max(0, min(100, augmented_row['H2'] + h2_change))\n",
        "\n",
        "            co_change = temp_change_pct * -0.2 * augmented_row['CO']\n",
        "            augmented_row['CO'] = max(0, min(100, augmented_row['CO'] + co_change))\n",
        "\n",
        "            if 'CO2' in augmented_row and 'CH4' in augmented_row:\n",
        "                current_sum = (augmented_row['H2'] + augmented_row['CO'] +\n",
        "                             augmented_row['CO2'] + augmented_row['CH4'])\n",
        "                if current_sum > 0:\n",
        "                    scale_factor = 100 / current_sum\n",
        "                    augmented_row['CO2'] = augmented_row['CO2'] * scale_factor\n",
        "                    augmented_row['CH4'] = augmented_row['CH4'] * scale_factor\n",
        "\n",
        "    # S/B ratio variation\n",
        "    if 'S/B' in augmented_row:\n",
        "        sb = augmented_row['S/B']\n",
        "        sb_variation = np.random.normal(1, 0.15)\n",
        "        augmented_row['S/B'] = max(0.1, sb * sb_variation)\n",
        "\n",
        "    # Small variations to other numerical parameters\n",
        "    for col in data_clean.select_dtypes(include=[np.number]).columns:\n",
        "        if col not in ['TYPES', 'Temperature (°C)', 'S/B', 'H2', 'CO', 'CO2', 'CH4']:\n",
        "            if col in augmented_row:\n",
        "                value = augmented_row[col]\n",
        "                if pd.notna(value) and value != 0:\n",
        "                    variation = np.random.normal(0, 0.05)\n",
        "                    augmented_row[col] = max(0, value * (1 + variation))\n",
        "\n",
        "    augmented_rows.append(augmented_row)\n",
        "\n",
        "# Combine original and augmented data\n",
        "augmented_df = pd.DataFrame(augmented_rows)\n",
        "synthetic_data = pd.concat([data_clean, augmented_df], ignore_index=True)\n",
        "\n",
        "# Ensure exact target_total samples\n",
        "if len(synthetic_data) > target_total:\n",
        "    synthetic_data = synthetic_data.iloc[:target_total]\n",
        "elif len(synthetic_data) < target_total:\n",
        "    additional_needed = target_total - len(synthetic_data)\n",
        "    additional_samples = augmented_df.sample(n=additional_needed, replace=True, random_state=42)\n",
        "    synthetic_data = pd.concat([synthetic_data, additional_samples], ignore_index=True)\n",
        "\n",
        "print(f\"\\nGenerated augmented dataset: {len(synthetic_data)} samples\")\n",
        "print(f\"  Original: {len(data_clean)} samples\")\n",
        "print(f\"  Augmented: {len(augmented_df)} samples\")\n",
        "print(f\"  Unique biomass types: {synthetic_data['TYPES'].nunique()}\")\n",
        "\n",
        "# Validate data quality\n",
        "print(\"\\nValidating data quality...\")\n",
        "\n",
        "if all(col in synthetic_data.columns for col in ['H2', 'CO', 'Temperature (°C)']):\n",
        "    orig_h2_co = data_clean['H2'].corr(data_clean['CO'])\n",
        "    synth_h2_co = synthetic_data['H2'].corr(synthetic_data['CO'])\n",
        "    orig_h2_temp = data_clean['H2'].corr(data_clean['Temperature (°C)'])\n",
        "    synth_h2_temp = synthetic_data['H2'].corr(synthetic_data['Temperature (°C)'])\n",
        "\n",
        "    print(f\"\\nCorrelation comparison:\")\n",
        "    print(f\"  H2-CO: Original={orig_h2_co:.3f}, Synthetic={synth_h2_co:.3f}\")\n",
        "    print(f\"  H2-Temp: Original={orig_h2_temp:.3f}, Synthetic={synth_h2_temp:.3f}\")\n",
        "\n",
        "    gas_cols = ['H2', 'CO', 'CO2', 'CH4']\n",
        "    if all(col in synthetic_data.columns for col in gas_cols):\n",
        "        orig_sum = data_clean[gas_cols].sum(axis=1).mean()\n",
        "        synth_sum = synthetic_data[gas_cols].sum(axis=1).mean()\n",
        "        print(f\"\\nGas composition sum:\")\n",
        "        print(f\"  Original mean: {orig_sum:.1f}%\")\n",
        "        print(f\"  Synthetic mean: {synth_sum:.1f}%\")\n",
        "\n",
        "print(\"\\nAugmentation complete!\")"
      ],
      "metadata": {
        "id": "ey_kPooyIgAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BIOMASS TYPE DISTRIBUTION - SYNTHETIC DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Group synthetic data by biomass type\n",
        "synth_type_groups = {}\n",
        "for biomass_type in synthetic_data['TYPES'].unique():\n",
        "    type_data = synthetic_data[synthetic_data['TYPES'] == biomass_type]\n",
        "    synth_type_groups[biomass_type] = len(type_data)\n",
        "\n",
        "print(f\"\\nBiomass types and experiment counts in SYNTHETIC data:\")\n",
        "print(f\"Total samples: {len(synthetic_data)}\")\n",
        "print(f\"Unique biomass types: {synthetic_data['TYPES'].nunique()}\")\n",
        "\n",
        "# Sort by count for better readability\n",
        "synth_type_groups_sorted = dict(sorted(synth_type_groups.items(),\n",
        "                                       key=lambda x: x[1], reverse=True))\n",
        "\n",
        "for biomass_type, count in synth_type_groups_sorted.items():\n",
        "    print(f\"  {biomass_type}: {count} experiments\")\n",
        "\n",
        "# Compare with original\n",
        "print(f\"\\n\" + \"-\"*60)\n",
        "print(\"DISTRIBUTION COMPARISON\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_list = []\n",
        "all_types = sorted(set(list(type_groups.keys()) + list(synth_type_groups.keys())))\n",
        "\n",
        "for biomass_type in all_types:\n",
        "    orig_count = type_groups.get(biomass_type, 0)\n",
        "    synth_count = synth_type_groups.get(biomass_type, 0)\n",
        "    increase = synth_count - orig_count\n",
        "    increase_pct = (increase / orig_count * 100) if orig_count > 0 else 0\n",
        "\n",
        "    comparison_list.append({\n",
        "        'Biomass_Type': biomass_type,\n",
        "        'Original': orig_count,\n",
        "        'Synthetic': synth_count,\n",
        "        'Increase': increase,\n",
        "        'Increase_%': f\"{increase_pct:.0f}%\" if orig_count > 0 else \"N/A\"\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_list)\n",
        "\n",
        "# Sort by original count (or synthetic count if no original)\n",
        "comparison_df = comparison_df.sort_values(by=['Original', 'Synthetic'], ascending=[False, False])\n",
        "\n",
        "print(\"\\nBiomass Type Distribution Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "puquAajBRCGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Quality Evaluation\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"QUALITY EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Check basic statistics\n",
        "print(\"\\n1. Statistical Comparison:\")\n",
        "\n",
        "key_params = ['C', 'H', 'O', 'Ash', 'H2', 'CO', 'CO2', 'CH4', 'Temperature (°C)', 'S/B']\n",
        "\n",
        "comparison_results = []\n",
        "for param in key_params:\n",
        "    if param in data_clean.columns and param in synthetic_data.columns:\n",
        "        orig_mean = data_clean[param].mean()\n",
        "        synth_mean = synthetic_data[param].mean()\n",
        "        mean_diff_pct = abs(orig_mean - synth_mean) / orig_mean * 100 if orig_mean != 0 else 0\n",
        "\n",
        "        orig_std = data_clean[param].std()\n",
        "        synth_std = synthetic_data[param].std()\n",
        "        std_diff_pct = abs(orig_std - synth_std) / orig_std * 100 if orig_std != 0 else 0\n",
        "\n",
        "        comparison_results.append({\n",
        "            'Parameter': param,\n",
        "            'Orig_Mean': f\"{orig_mean:.2f}\",\n",
        "            'Synth_Mean': f\"{synth_mean:.2f}\",\n",
        "            'Mean_Diff_%': f\"{mean_diff_pct:.1f}\",\n",
        "            'Orig_STD': f\"{orig_std:.2f}\",\n",
        "            'Synth_STD': f\"{synth_std:.2f}\",\n",
        "            'STD_Diff_%': f\"{std_diff_pct:.1f}\"\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# 2. Check biomass type distribution\n",
        "print(\"\\n2. Type Distribution Check:\")\n",
        "\n",
        "original_types = data_clean['TYPES'].value_counts(normalize=True).sort_index()\n",
        "synthetic_types = synthetic_data['TYPES'].value_counts(normalize=True).sort_index()\n",
        "\n",
        "type_comparison = pd.DataFrame({\n",
        "    'Original': original_types,\n",
        "    'Synthetic': synthetic_types\n",
        "}).fillna(0)\n",
        "\n",
        "print(\"\\nTop 10 biomass type comparison:\")\n",
        "print(type_comparison.head(10))\n",
        "\n",
        "coverage = len(set(data_clean['TYPES']) & set(synthetic_data['TYPES'])) / len(set(data_clean['TYPES'])) * 100\n",
        "print(f\"\\nType coverage: {coverage:.1f}%\")\n",
        "\n",
        "# 3. Check correlation patterns\n",
        "print(\"\\n3. Correlation Check:\")\n",
        "\n",
        "numerical_cols = data_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "common_numerical = [col for col in numerical_cols if col in synthetic_data.columns]\n",
        "\n",
        "if len(common_numerical) > 1:\n",
        "    corr_original = data_clean[common_numerical].corr()\n",
        "    corr_synthetic = synthetic_data[common_numerical].corr()\n",
        "\n",
        "    # Calculate average difference in correlations\n",
        "    corr_diff = (corr_original - corr_synthetic).abs().mean().mean()\n",
        "    print(f\"Average correlation difference: {corr_diff:.4f}\")\n",
        "\n",
        "    # Check specific important relationships\n",
        "    key_pairs = [('H2', 'Temperature (°C)'), ('H2', 'CO'), ('CO', 'CO2')]\n",
        "    for col1, col2 in key_pairs:\n",
        "        if col1 in common_numerical and col2 in common_numerical:\n",
        "            orig_corr = corr_original.loc[col1, col2]\n",
        "            synth_corr = corr_synthetic.loc[col1, col2]\n",
        "            diff = abs(orig_corr - synth_corr)\n",
        "            print(f\"  {col1}-{col2}: Original={orig_corr:.3f}, Synthetic={synth_corr:.3f}, Difference={diff:.3f}\")\n",
        "\n",
        "# 4. Check gas composition totals\n",
        "print(\"\\n4. Gas Composition Check:\")\n",
        "\n",
        "if all(col in synthetic_data.columns for col in gas_cols):\n",
        "    synth_gas_sum = synthetic_data[gas_cols].sum(axis=1)\n",
        "    print(f\"Gas total sum:\")\n",
        "    print(f\"  Average: {synth_gas_sum.mean():.1f}%\")\n",
        "    print(f\"  Min-Max: [{synth_gas_sum.min():.1f}%, {synth_gas_sum.max():.1f}%]\")\n",
        "\n",
        "    # Count samples with unrealistic totals\n",
        "    unrealistic = (synth_gas_sum < 80) | (synth_gas_sum > 120)\n",
        "    print(f\"  Unrealistic totals: {unrealistic.sum()} samples\")"
      ],
      "metadata": {
        "id": "f--OreD8IgCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Visual Comparisons\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CREATING VISUAL COMPARISONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Temperature vs H2 Production\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot original data\n",
        "plt.scatter(data_clean['Temperature (°C)'], data_clean['H2'],\n",
        "            alpha=0.7, label='Original', color='blue', s=60, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot synthetic data\n",
        "plt.scatter(synthetic_data['Temperature (°C)'], synthetic_data['H2'],\n",
        "            alpha=0.4, label='Synthetic', color='red', s=30)\n",
        "\n",
        "# Add trend lines\n",
        "z_orig = np.polyfit(data_clean['Temperature (°C)'], data_clean['H2'], 1)\n",
        "p_orig = np.poly1d(z_orig)\n",
        "x_range = np.linspace(data_clean['Temperature (°C)'].min(), data_clean['Temperature (°C)'].max(), 100)\n",
        "plt.plot(x_range, p_orig(x_range), 'blue', linewidth=2, linestyle='-', label='Original Trend')\n",
        "\n",
        "z_synth = np.polyfit(synthetic_data['Temperature (°C)'], synthetic_data['H2'], 1)\n",
        "p_synth = np.poly1d(z_synth)\n",
        "plt.plot(x_range, p_synth(x_range), 'red', linewidth=2, linestyle='--', label='Synthetic Trend')\n",
        "\n",
        "plt.xlabel('Temperature (°C)', fontsize=12)\n",
        "plt.ylabel('H₂ Production (%)', fontsize=12)\n",
        "plt.title('Temperature vs H₂ Production', fontsize=14)\n",
        "\n",
        "# Add grid and legend\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "plt.legend(fontsize=10)\n",
        "\n",
        "# Set axis limits\n",
        "plt.xlim(data_clean['Temperature (°C)'].min()-10, data_clean['Temperature (°C)'].max()+10)\n",
        "plt.ylim(min(data_clean['H2'].min(), synthetic_data['H2'].min())-2,\n",
        "         max(data_clean['H2'].max(), synthetic_data['H2'].max())+2)\n",
        "\n",
        "# Add correlation values\n",
        "orig_corr = data_clean['Temperature (°C)'].corr(data_clean['H2'])\n",
        "synth_corr = synthetic_data['Temperature (°C)'].corr(synthetic_data['H2'])\n",
        "plt.text(0.02, 0.98, f'Original r = {orig_corr:.3f}',\n",
        "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
        "plt.text(0.02, 0.92, f'Synthetic r = {synth_corr:.3f}',\n",
        "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('temperature_vs_h2.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 2. H2 Production by Biomass Type\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Combine data for boxplot\n",
        "plot_data = pd.concat([\n",
        "    data_clean[['TYPES', 'H2']].assign(Source='Original'),\n",
        "    synthetic_data[['TYPES', 'H2']].assign(Source='Synthetic')\n",
        "])\n",
        "\n",
        "# Select top 8 most common types\n",
        "top_types = data_clean['TYPES'].value_counts().head(8).index\n",
        "plot_data = plot_data[plot_data['TYPES'].isin(top_types)]\n",
        "\n",
        "sns.boxplot(data=plot_data, x='TYPES', y='H2', hue='Source',\n",
        "            palette={'Original': 'blue', 'Synthetic': 'red'})\n",
        "plt.title('H₂ Production by Biomass Type', fontsize=14)\n",
        "plt.xlabel('Biomass Type', fontsize=12)\n",
        "plt.ylabel('H₂ Production (%)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Data Source', fontsize=10)\n",
        "\n",
        "plt.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig('h2_by_type.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ch3l6NCfLbaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7.5: Gas Composition Analysis\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GAS COMPOSITION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Check gas correlations\n",
        "print(\"\\n1. Gas Composition Correlations:\")\n",
        "\n",
        "gas_cols = ['H2', 'CO', 'CO2', 'CH4']\n",
        "if all(col in data_clean.columns for col in gas_cols):\n",
        "    orig_corr = data_clean[gas_cols].corr()\n",
        "    synth_corr = synthetic_data[gas_cols].corr()\n",
        "\n",
        "    print(f\"\\n   H2-CO correlation:\")\n",
        "    print(f\"     Original: {orig_corr.loc['H2', 'CO']:.3f}\")\n",
        "    print(f\"     Synthetic: {synth_corr.loc['H2', 'CO']:.3f}\")\n",
        "\n",
        "    print(f\"\\n   H2-CO2 correlation:\")\n",
        "    print(f\"     Original: {orig_corr.loc['H2', 'CO2']:.3f}\")\n",
        "    print(f\"     Synthetic: {synth_corr.loc['H2', 'CO2']:.3f}\")\n",
        "\n",
        "# 2. Check gas distributions\n",
        "print(\"\\n2. Gas Distribution Comparison:\")\n",
        "\n",
        "critical_gases = ['CO', 'CH4', 'CO2']\n",
        "for gas in critical_gases:\n",
        "    if gas in data_clean.columns:\n",
        "        orig_mean = data_clean[gas].mean()\n",
        "        synth_mean = synthetic_data[gas].mean()\n",
        "        mean_diff = abs(orig_mean - synth_mean)\n",
        "\n",
        "        print(f\"\\n   {gas}:\")\n",
        "        print(f\"     Original mean: {orig_mean:.1f}%\")\n",
        "        print(f\"     Synthetic mean: {synth_mean:.1f}%\")\n",
        "        print(f\"     Difference: {mean_diff:.1f}%\")\n",
        "\n",
        "# 3. Check temperature relationship\n",
        "if 'Temperature (°C)' in data_clean.columns:\n",
        "    print(\"\\n3. Temperature-H2 Relationship:\")\n",
        "    temp_h2_orig = data_clean['Temperature (°C)'].corr(data_clean['H2'])\n",
        "    temp_h2_synth = synthetic_data['Temperature (°C)'].corr(synthetic_data['H2'])\n",
        "\n",
        "    print(f\"   Original correlation: {temp_h2_orig:.3f}\")\n",
        "    print(f\"   Synthetic correlation: {temp_h2_synth:.3f}\")"
      ],
      "metadata": {
        "id": "Vt5QuM6wQslj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# STEP 8: Machine Learning Efficacy Test\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MACHINE LEARNING EFFICACY TEST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "target_column = 'H2'\n",
        "\n",
        "# Prepare data for ML\n",
        "def prepare_for_ml(data, target_col):\n",
        "    ml_data = data.copy()\n",
        "\n",
        "    # One-hot encode biomass types\n",
        "    if 'TYPES' in ml_data.columns:\n",
        "        types_dummies = pd.get_dummies(ml_data['TYPES'], prefix='TYPE')\n",
        "        ml_data = pd.concat([ml_data, types_dummies], axis=1)\n",
        "        ml_data.drop('TYPES', axis=1, inplace=True)\n",
        "\n",
        "    # Convert to numeric\n",
        "    ml_data = ml_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with missing target\n",
        "    ml_data = ml_data.dropna(subset=[target_col])\n",
        "    ml_data = ml_data.dropna(thresh=len(ml_data.columns) - 5)\n",
        "\n",
        "    return ml_data\n",
        "\n",
        "print(\"Preparing datasets...\")\n",
        "ml_original = prepare_for_ml(data_clean, target_column)\n",
        "ml_synthetic = prepare_for_ml(synthetic_data, target_column)\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"Original: {ml_original.shape}\")\n",
        "print(f\"Synthetic: {ml_synthetic.shape}\")\n",
        "\n",
        "# Align columns\n",
        "common_cols = list(set(ml_original.columns) & set(ml_synthetic.columns))\n",
        "if target_column in common_cols:\n",
        "    common_cols.remove(target_column)\n",
        "\n",
        "ml_original = ml_original[common_cols + [target_column]]\n",
        "ml_synthetic = ml_synthetic[common_cols + [target_column]]\n",
        "\n",
        "print(f\"\\nAfter column alignment:\")\n",
        "print(f\"Original: {ml_original.shape}\")\n",
        "print(f\"Synthetic: {ml_synthetic.shape}\")\n",
        "print(f\"Features: {len(common_cols)}\")\n",
        "\n",
        "# Split original data\n",
        "X = ml_original.drop(target_column, axis=1)\n",
        "y = ml_original[target_column]\n",
        "\n",
        "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare synthetic training data\n",
        "X_train_synth = ml_synthetic.drop(target_column, axis=1).iloc[:len(X_train_orig)]\n",
        "y_train_synth = ml_synthetic[target_column].iloc[:len(y_train_orig)]\n",
        "\n",
        "print(f\"\\nTraining set sizes:\")\n",
        "print(f\"Original training: {X_train_orig.shape}\")\n",
        "print(f\"Synthetic training: {X_train_synth.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Train models\n",
        "print(\"\\nTraining models...\")\n",
        "\n",
        "# Model on original data\n",
        "rf_original = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
        "rf_original.fit(X_train_orig, y_train_orig)\n",
        "y_pred_orig = rf_original.predict(X_test)\n",
        "\n",
        "# Model on synthetic data\n",
        "rf_synthetic = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
        "rf_synthetic.fit(X_train_synth, y_train_synth)\n",
        "y_pred_synth = rf_synthetic.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "r2_orig = r2_score(y_test, y_pred_orig)\n",
        "r2_synth = r2_score(y_test, y_pred_synth)\n",
        "rmse_orig = np.sqrt(mean_squared_error(y_test, y_pred_orig))\n",
        "rmse_synth = np.sqrt(mean_squared_error(y_test, y_pred_synth))\n",
        "mae_orig = mean_absolute_error(y_test, y_pred_orig)\n",
        "mae_synth = mean_absolute_error(y_test, y_pred_synth)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\nOriginal Data Model:\")\n",
        "print(f\"  R² Score: {r2_orig:.4f}\")\n",
        "print(f\"  RMSE: {rmse_orig:.4f}\")\n",
        "print(f\"  MAE: {mae_orig:.4f}\")\n",
        "\n",
        "print(f\"\\nSynthetic Data Model:\")\n",
        "print(f\"  R² Score: {r2_synth:.4f}\")\n",
        "print(f\"  RMSE: {rmse_synth:.4f}\")\n",
        "print(f\"  MAE: {mae_synth:.4f}\")\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  R² Difference: {abs(r2_orig - r2_synth):.4f}\")\n",
        "print(f\"  Performance Ratio: {r2_synth/r2_orig:.3f}\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\nTop 10 Important Features (Original Model):\")\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train_orig.columns,\n",
        "    'Importance': rf_original.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "for i, (feature, importance) in enumerate(zip(feature_importances['Feature'][:10],\n",
        "                                              feature_importances['Importance'][:10])):\n",
        "    print(f\"  {i+1:2d}. {feature:25s}: {importance:.4f}\")\n",
        "\n",
        "# Performance summary\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "\n",
        "if r2_orig > 0:\n",
        "    performance_ratio = r2_synth / r2_orig\n",
        "\n",
        "    if r2_synth > 0.5:\n",
        "        print(f\"  Synthetic model R² > 0.5: Acceptable\")\n",
        "    else:\n",
        "        print(f\"  Synthetic model R² < 0.5: Low\")\n",
        "\n",
        "    if performance_ratio > 0.8:\n",
        "        print(f\"  Performance ratio > 0.8: Good\")\n",
        "    elif performance_ratio > 0.6:\n",
        "        print(f\"  Performance ratio 0.6-0.8: Moderate\")\n",
        "    else:\n",
        "        print(f\"  Performance ratio < 0.6: Poor\")\n",
        "\n",
        "print(\"\\nML test completed!\")"
      ],
      "metadata": {
        "id": "1XiAt06PLbeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Final Quality Validation\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL QUALITY VALIDATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check key parameter ranges\n",
        "print(\"\\nParameter Range Comparison:\")\n",
        "\n",
        "key_params = ['C', 'H', 'H2', 'Temperature (°C)']\n",
        "\n",
        "for param in key_params:\n",
        "    if param in data_clean.columns:\n",
        "        orig_min = data_clean[param].min()\n",
        "        orig_max = data_clean[param].max()\n",
        "        synth_min = synthetic_data[param].min()\n",
        "        synth_max = synthetic_data[param].max()\n",
        "\n",
        "        print(f\"\\n{param}:\")\n",
        "        print(f\"  Original:  [{orig_min:.1f}, {orig_max:.1f}]\")\n",
        "        print(f\"  Synthetic: [{synth_min:.1f}, {synth_max:.1f}]\")\n",
        "\n",
        "        # Check if ranges are preserved\n",
        "        if synth_min >= orig_min and synth_max <= orig_max:\n",
        "            print(f\"  Status: Within original range\")\n",
        "        else:\n",
        "            print(f\"  Status: Outside original range\")"
      ],
      "metadata": {
        "id": "xUzuxDcNLbg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Round Synthetic Data Values\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ROUNDING SYNTHETIC DATA VALUES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Round all numerical columns\n",
        "print(\"Rounding numerical values...\")\n",
        "\n",
        "for col in synthetic_data.select_dtypes(include=[np.number]).columns:\n",
        "    synthetic_data[col] = synthetic_data[col].round(3)\n",
        "\n",
        "print(f\"Rounded {synthetic_data.select_dtypes(include=[np.number]).shape[1]} columns\")\n",
        "print(\"\\nSample of rounded data:\")\n",
        "print(synthetic_data.head(3))"
      ],
      "metadata": {
        "id": "FioRbCuHDiSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Save Results\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "import os\n",
        "SAVE_DIR = \"/content/drive/MyDrive/ML_Lab_Datasets/\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save synthetic data\n",
        "synthetic_data.to_csv('final_synthetic_biomass_dataset.csv', index=False)\n",
        "synthetic_data.to_excel('final_synthetic_biomass_dataset.xlsx', index=False)\n",
        "\n",
        "# Save the trained synthesizer\n",
        "synthesizer.save('final_biomass_synthesizer.pkl')\n",
        "\n",
        "# Save quality report\n",
        "with open('final_synthetic_data_quality_report.txt', 'w') as f:\n",
        "    f.write(\"BIOMASS GASIFICATION SYNTHETIC DATA QUALITY REPORT\\n\")\n",
        "    f.write(\"=\" * 55 + \"\\n\\n\")\n",
        "    f.write(f\"Original data size: {data_clean.shape}\\n\")\n",
        "    f.write(f\"Synthetic data size: {synthetic_data.shape}\\n\")\n",
        "    f.write(f\"Data expansion: {len(synthetic_data)/len(data_clean):.1f}x\\n\\n\")\n",
        "\n",
        "    f.write(\"TYPE DISTRIBUTION:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(f\"Original types: {data_clean['TYPES'].nunique()}\\n\")\n",
        "    f.write(f\"Synthetic types: {synthetic_data['TYPES'].nunique()}\\n\")\n",
        "    f.write(f\"Type coverage: {coverage:.1f}%\\n\\n\")\n",
        "\n",
        "    f.write(\"KEY PARAMETER STATISTICS:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "    if not comparison_df.empty:\n",
        "        for _, row in comparison_df.iterrows():\n",
        "            f.write(f\"{row['Parameter']}: Original={row['Orig_Mean']}, Synthetic={row['Synth_Mean']}, Diff={row['Mean_Diff_%']}%\\n\")\n",
        "\n",
        "    f.write(\"\\nMACHINE LEARNING PERFORMANCE:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "    if 'r2_orig' in locals() and 'r2_synth' in locals():\n",
        "        f.write(f\"R² (Original data): {r2_orig:.4f}\\n\")\n",
        "        f.write(f\"R² (Synthetic data): {r2_synth:.4f}\\n\")\n",
        "        f.write(f\"Performance difference: {abs(r2_orig - r2_synth):.4f}\\n\")\n",
        "        if r2_orig != 0:\n",
        "            f.write(f\"Performance ratio: {r2_synth/r2_orig:.2f}\\n\")\n",
        "\n",
        "print(\"\\nFiles saved:\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PROCESS COMPLETED\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original dataset: {len(data_clean)} rows, {data_clean['TYPES'].nunique()} types\")\n",
        "print(f\"Synthetic dataset: {len(synthetic_data)} rows, {synthetic_data['TYPES'].nunique()} types\")\n",
        "print(f\"Data expansion: {len(synthetic_data)/len(data_clean):.1f}x\")\n",
        "print(f\"Type coverage: {coverage:.1f}%\")\n",
        "\n",
        "print(\"\\nSample of synthetic data:\")\n",
        "print(synthetic_data.head(3))"
      ],
      "metadata": {
        "id": "UOicTuLmLbjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}